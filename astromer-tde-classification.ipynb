{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T10:43:40.716697Z",
     "iopub.status.busy": "2025-12-24T10:43:40.715985Z",
     "iopub.status.idle": "2025-12-24T10:43:51.220521Z",
     "shell.execute_reply": "2025-12-24T10:43:51.219701Z",
     "shell.execute_reply.started": "2025-12-24T10:43:40.716663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running for first time)\n",
    "# !pip install -q extinction==0.4.7\n",
    "# !pip install -q optuna\n",
    "# !pip install -q catboost\n",
    "\n",
    "# Clone Astromer repository (if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('./astromer_repo'):\n",
    "    !git clone https://github.com/astromer-science/main-code.git astromer_repo\n",
    "    print(\"âœ“ Astromer repo cloned\")\n",
    "else:\n",
    "    print(\"âœ“ Astromer repo already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "astromer_path = os.path.abspath('./astromer_repo/src')\n",
    "if astromer_path not in sys.path:\n",
    "    sys.path.insert(0, astromer_path)\n",
    "\n",
    "print(\"âœ“ Dependencies ready\")\n",
    "print(\"âœ“ Astromer path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T11:14:04.577075Z",
     "iopub.status.idle": "2025-12-24T11:14:04.577304Z",
     "shell.execute_reply": "2025-12-24T11:14:04.577208Z",
     "shell.execute_reply.started": "2025-12-24T11:14:04.577194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Extinction correction\n",
    "from extinction import fitzpatrick99\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# For later stages\n",
    "# import astromer  # Will be used after installation\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:47:26.396207Z",
     "iopub.status.busy": "2025-12-24T09:47:26.395758Z",
     "iopub.status.idle": "2025-12-24T09:47:26.402032Z",
     "shell.execute_reply": "2025-12-24T09:47:26.401204Z",
     "shell.execute_reply.started": "2025-12-24T09:47:26.396184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup paths for local/vast.ai environment\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base paths (adjust if needed)\n",
    "BASE_PATH = \"./data\"  # Dataset location\n",
    "WORKING_DIR = \".\"     # Current directory\n",
    "\n",
    "# Create output directories\n",
    "PREPROCESSED_DIR = f\"{WORKING_DIR}/preprocessed_data\"\n",
    "EMBEDDINGS_DIR = f\"{WORKING_DIR}/embeddings\"\n",
    "MODELS_DIR = f\"{WORKING_DIR}/models\"\n",
    "CHECKPOINTS_DIR = f\"{WORKING_DIR}/checkpoints\"\n",
    "\n",
    "for dir_path in [PREPROCESSED_DIR, EMBEDDINGS_DIR, MODELS_DIR, CHECKPOINTS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Directories configured:\")\n",
    "print(f\"  - Data: {BASE_PATH}\")\n",
    "print(f\"  - Preprocessed data: {PREPROCESSED_DIR}\")\n",
    "print(f\"  - Embeddings: {EMBEDDINGS_DIR}\")\n",
    "print(f\"  - Models: {MODELS_DIR}\")\n",
    "print(f\"  - Checkpoints: {CHECKPOINTS_DIR}\")\n",
    "\n",
    "# Verify data exists\n",
    "if os.path.exists(f\"{BASE_PATH}/train_log.csv\"):\n",
    "    print(\"\\nâœ“ Dataset found\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Dataset not found. Please upload data to ./data/ directory\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"  ./data/train_log.csv\")\n",
    "    print(\"  ./data/test_log.csv\")\n",
    "    print(\"  ./data/train/ (with split folders)\")\n",
    "    print(\"  ./data/test/ (with split folders)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:47:26.404000Z",
     "iopub.status.busy": "2025-12-24T09:47:26.403740Z",
     "iopub.status.idle": "2025-12-24T09:47:26.516506Z",
     "shell.execute_reply": "2025-12-24T09:47:26.515869Z",
     "shell.execute_reply.started": "2025-12-24T09:47:26.403973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load train/test metadata\n",
    "train_df = pd.read_csv(f\"{BASE_PATH}/train_log.csv\")\n",
    "test_df = pd.read_csv(f\"{BASE_PATH}/test_log.csv\")\n",
    "\n",
    "print(f\"Training objects: {len(train_df)}\")\n",
    "print(f\"Test objects: {len(test_df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df['target'].value_counts())\n",
    "print(f\"\\nImbalance ratio: {train_df['target'].value_counts()[0] / train_df['target'].value_counts()[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:47:26.517618Z",
     "iopub.status.busy": "2025-12-24T09:47:26.517329Z",
     "iopub.status.idle": "2025-12-24T09:47:26.532993Z",
     "shell.execute_reply": "2025-12-24T09:47:26.532283Z",
     "shell.execute_reply.started": "2025-12-24T09:47:26.517594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BANDS = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "EFF_WAVELENGTHS = {\n",
    "    \"u\": 3641, \"g\": 4704, \"r\": 6155, \n",
    "    \"i\": 7504, \"z\": 8695, \"y\": 10056\n",
    "}\n",
    "R_V = 3.1\n",
    "MAX_OBSERVATIONS = 200  # Astromer input length\n",
    "FLUX_ZERO = 1.0  # Softening flux for asinh magnitudes (Î¼Jy)\n",
    "ZERO_POINT = 23.9  # AB magnitude zero point for Î¼Jy\n",
    "\n",
    "def flux_to_magnitude(flux, flux_err):\n",
    "    \"\"\"\n",
    "    Convert flux (Î¼Jy) to Asinh magnitude\n",
    "    \n",
    "    Standard approach for handling negative flux measurements:\n",
    "    m_asinh = -2.5/ln(10) * asinh(flux / (2*flux_0)) + zero_point\n",
    "    \n",
    "    This function is well-defined for negative flux and smoothly\n",
    "    transitions through zero, preserving information about\n",
    "    negative measurements (common in astronomical data).\n",
    "    \n",
    "    Reference: Lupton et al. 1999, AJ 118:1406\n",
    "    \n",
    "    Args:\n",
    "        flux: Flux in microjanskys (can be negative)\n",
    "        flux_err: Flux uncertainty in microjanskys\n",
    "    \n",
    "    Returns:\n",
    "        magnitude: Asinh magnitude\n",
    "        magnitude_error: Propagated magnitude error\n",
    "    \"\"\"\n",
    "    # Asinh magnitude - well-defined for negative flux\n",
    "    b = 2 * FLUX_ZERO  # Softening parameter = 2 Î¼Jy\n",
    "    magnitude = -2.5 / np.log(10) * np.arcsinh(flux / b) + ZERO_POINT\n",
    "    \n",
    "    # Error propagation for asinh magnitude:\n",
    "    # dm/df = (2.5 / ln(10)) * (1/b) / sqrt(1 + (f/b)^2)\n",
    "    mag_err = (2.5 / np.log(10)) * (flux_err / b) / np.sqrt(1 + (flux / b)**2)\n",
    "    \n",
    "    return magnitude, mag_err\n",
    "\n",
    "def correct_extinction(flux, flux_err, filter_name, ebv):\n",
    "    \"\"\"\n",
    "    Apply extinction correction using Fitzpatrick99 law\n",
    "    \n",
    "    Note: This preserves the sign of flux (negative flux remains negative)\n",
    "    which is physically correct for background-subtracted measurements.\n",
    "    \n",
    "    Returns corrected flux and error\n",
    "    \"\"\"\n",
    "    wavelength = EFF_WAVELENGTHS[filter_name]\n",
    "    A_lambda = fitzpatrick99(np.array([wavelength]), ebv * R_V)[0]\n",
    "    scale = 10 ** (A_lambda / 2.5)\n",
    "    \n",
    "    flux_true = flux * scale\n",
    "    flux_err_true = flux_err * scale\n",
    "    \n",
    "    return flux_true, flux_err_true\n",
    "\n",
    "def pad_or_truncate(time, mag, mag_err, max_len=MAX_OBSERVATIONS):\n",
    "    \"\"\"\n",
    "    Pad (with zeros) or truncate light curve to fixed length\n",
    "    \n",
    "    For truncation: Uses peak-centered window strategy optimal for TDE detection.\n",
    "    Finds the brightness peak (minimum magnitude) and extracts a window around it,\n",
    "    preserving the most informative temporal features (rise, peak, early decay).\n",
    "    \n",
    "    Returns:\n",
    "        padded arrays, mask array (1=real, 0=padded)\n",
    "    \"\"\"\n",
    "    n_obs = len(time)\n",
    "    \n",
    "    if n_obs == 0:\n",
    "        # Completely empty band - return all zeros\n",
    "        return (\n",
    "            np.zeros(max_len),\n",
    "            np.zeros(max_len),\n",
    "            np.zeros(max_len),\n",
    "            np.zeros(max_len, dtype=bool)\n",
    "        )\n",
    "    \n",
    "    if n_obs >= max_len:\n",
    "        # Truncate - use peak-centered window\n",
    "        # Find peak (minimum magnitude = brightest point)\n",
    "        peak_idx = np.argmin(mag)\n",
    "        \n",
    "        # Calculate window boundaries centered on peak\n",
    "        half_window = max_len // 2\n",
    "        start_idx = max(0, peak_idx - half_window)\n",
    "        end_idx = min(n_obs, peak_idx + half_window)\n",
    "        \n",
    "        # Adjust if window hits boundary\n",
    "        if end_idx - start_idx < max_len:\n",
    "            if start_idx == 0:\n",
    "                end_idx = min(n_obs, start_idx + max_len)\n",
    "            else:\n",
    "                start_idx = max(0, end_idx - max_len)\n",
    "        \n",
    "        # Extract window\n",
    "        indices = np.arange(start_idx, start_idx + max_len)\n",
    "        return (\n",
    "            time[indices],\n",
    "            mag[indices],\n",
    "            mag_err[indices],\n",
    "            np.ones(max_len, dtype=bool)\n",
    "        )\n",
    "    else:\n",
    "        # Pad with zeros\n",
    "        time_padded = np.zeros(max_len)\n",
    "        mag_padded = np.zeros(max_len)\n",
    "        mag_err_padded = np.zeros(max_len)\n",
    "        mask = np.zeros(max_len, dtype=bool)\n",
    "        \n",
    "        time_padded[:n_obs] = time\n",
    "        mag_padded[:n_obs] = mag\n",
    "        mag_err_padded[:n_obs] = mag_err\n",
    "        mask[:n_obs] = True\n",
    "        \n",
    "        return time_padded, mag_padded, mag_err_padded, mask\n",
    "\n",
    "def normalize_lightcurve(time, mag, mag_err, mask, redshift):\n",
    "    \"\"\"\n",
    "    Apply zero-mean normalization (Astromer requirement)\n",
    "    + Rest-frame time correction for time dilation\n",
    "    Only normalize over real observations (mask=True)\n",
    "    \n",
    "    Args:\n",
    "        redshift: Cosmological redshift (z) for time dilation correction\n",
    "                  t_rest = t_obs / (1 + z)\n",
    "    \"\"\"\n",
    "    if mask.sum() == 0:\n",
    "        return time, mag, mag_err\n",
    "    \n",
    "    # Normalize only real observations\n",
    "    time_norm = time.copy()\n",
    "    mag_norm = mag.copy()\n",
    "    mag_err_norm = mag_err.copy()\n",
    "    \n",
    "    # Zero-mean normalization + rest-frame correction\n",
    "    # Time dilation: observed time = rest time Ã— (1 + z)\n",
    "    # So: rest time = observed time / (1 + z)\n",
    "    time_norm[mask] = (time[mask] - time[mask].mean()) / (1 + redshift)\n",
    "    mag_norm[mask] = mag[mask] - mag[mask].mean()\n",
    "    # Don't normalize errors - keep original scale\n",
    "    \n",
    "    return time_norm, mag_norm, mag_err_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Process Light Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:47:26.534360Z",
     "iopub.status.busy": "2025-12-24T09:47:26.533983Z",
     "iopub.status.idle": "2025-12-24T09:47:26.700531Z",
     "shell.execute_reply": "2025-12-24T09:47:26.699905Z",
     "shell.execute_reply.started": "2025-12-24T09:47:26.534329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_object(object_id, split, ebv, redshift, is_train=True):\n",
    "    \"\"\"\n",
    "    Preprocess a single object:\n",
    "    1. Load light curve\n",
    "    2. Apply extinction correction\n",
    "    3. Convert flux to magnitude\n",
    "    4. Split by bands\n",
    "    5. Pad/truncate to 200 observations\n",
    "    6. Normalize (with rest-frame time correction for redshift)\n",
    "    \n",
    "    Args:\n",
    "        redshift: Cosmological redshift (z) for time dilation correction\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys: 'u', 'g', 'r', 'i', 'z', 'y'\n",
    "        Each value is dict: {'time': array, 'mag': array, 'mag_err': array, 'mask': array}\n",
    "    \"\"\"\n",
    "    # Load light curve\n",
    "    filename = \"train_full_lightcurves.csv\" if is_train else \"test_full_lightcurves.csv\"\n",
    "    lc_path = f\"{BASE_PATH}/{split}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        lc_df = pd.read_csv(lc_path)\n",
    "        lc_df = lc_df[lc_df['object_id'] == object_id]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {object_id}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for band in BANDS:\n",
    "        band_df = lc_df[lc_df['Filter'] == band].copy()\n",
    "        \n",
    "        if len(band_df) == 0:\n",
    "            # Empty band - all zeros\n",
    "            result[band] = {\n",
    "                'time': np.zeros(MAX_OBSERVATIONS),\n",
    "                'mag': np.zeros(MAX_OBSERVATIONS),\n",
    "                'mag_err': np.zeros(MAX_OBSERVATIONS),\n",
    "                'mask': np.zeros(MAX_OBSERVATIONS, dtype=bool)\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Sort by time\n",
    "        band_df = band_df.sort_values('Time (MJD)')\n",
    "        \n",
    "        # Apply extinction correction\n",
    "        flux_true, flux_err_true = correct_extinction(\n",
    "            band_df['Flux'].values,\n",
    "            band_df['Flux_err'].values,\n",
    "            band,\n",
    "            ebv\n",
    "        )\n",
    "        \n",
    "        # Convert to magnitude\n",
    "        mag, mag_err = flux_to_magnitude(flux_true, flux_err_true)\n",
    "        \n",
    "        # Handle invalid values\n",
    "        valid_mask = np.isfinite(mag) & np.isfinite(mag_err)\n",
    "        time = band_df['Time (MJD)'].values[valid_mask]\n",
    "        mag = mag[valid_mask]\n",
    "        mag_err = mag_err[valid_mask]\n",
    "        \n",
    "        # Pad/truncate\n",
    "        time_pad, mag_pad, mag_err_pad, mask = pad_or_truncate(time, mag, mag_err)\n",
    "        \n",
    "        # Normalize (with rest-frame correction for time dilation)\n",
    "        time_norm, mag_norm, mag_err_norm = normalize_lightcurve(\n",
    "            time_pad, mag_pad, mag_err_pad, mask, redshift\n",
    "        )\n",
    "        \n",
    "        result[band] = {\n",
    "            'time': time_norm,\n",
    "            'mag': mag_norm,\n",
    "            'mag_err': mag_err_norm,\n",
    "            'mask': mask\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test on a few objects\n",
    "print(\"Testing preprocessing on sample objects...\")\n",
    "sample_ids = train_df.sample(3)\n",
    "\n",
    "for idx, row in sample_ids.iterrows():\n",
    "    processed = preprocess_object(row['object_id'], row['split'], row['EBV'], row['Z'], is_train=True)\n",
    "    if processed:\n",
    "        print(f\"\\n{row['object_id']} (target={row['target']}, z={row['Z']:.3f}):\")\n",
    "        for band in BANDS:\n",
    "            n_real = processed[band]['mask'].sum()\n",
    "            print(f\"  {band}: {n_real}/200 real observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Batch Process All Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:47:26.701615Z",
     "iopub.status.busy": "2025-12-24T09:47:26.701381Z",
     "iopub.status.idle": "2025-12-24T09:53:33.651309Z",
     "shell.execute_reply": "2025-12-24T09:53:33.650617Z",
     "shell.execute_reply.started": "2025-12-24T09:47:26.701595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_all_objects(df, is_train=True, save=True):\n",
    "    \"\"\"\n",
    "    Process all objects and save to disk\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping object_id -> processed data\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    failed = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {'train' if is_train else 'test'}\"):\n",
    "        object_id = row['object_id']\n",
    "        processed = preprocess_object(object_id, row['split'], row['EBV'], row['Z'], is_train)\n",
    "        \n",
    "        if processed is None:\n",
    "            failed.append(object_id)\n",
    "            continue\n",
    "        \n",
    "        all_data[object_id] = processed\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nFailed to process {len(failed)} objects: {failed[:10]}...\")\n",
    "    \n",
    "    # Save to disk\n",
    "    if save:\n",
    "        dataset_name = \"train\" if is_train else \"test\"\n",
    "        save_path = f\"{PREPROCESSED_DIR}/{dataset_name}_preprocessed.pkl\"\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(all_data, f)\n",
    "        \n",
    "        print(f\"Saved {len(all_data)} objects to {save_path}\")\n",
    "        print(f\"File size: {os.path.getsize(save_path) / (1024**2):.1f} MB\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_data = process_all_objects(train_df, is_train=True, save=True)\n",
    "\n",
    "# Process test data\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_data = process_all_objects(test_df, is_train=False, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Verify Preprocessing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:53:36.668859Z",
     "iopub.status.busy": "2025-12-24T09:53:36.668569Z",
     "iopub.status.idle": "2025-12-24T09:53:41.162683Z",
     "shell.execute_reply": "2025-12-24T09:53:41.162006Z",
     "shell.execute_reply.started": "2025-12-24T09:53:36.668834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze preprocessing results\n",
    "def analyze_preprocessing(data_dict, df):\n",
    "    \"\"\"\n",
    "    Analyze processed data quality\n",
    "    \"\"\"\n",
    "    band_obs_counts = {band: [] for band in BANDS}\n",
    "    \n",
    "    for object_id, processed in data_dict.items():\n",
    "        for band in BANDS:\n",
    "            n_obs = processed[band]['mask'].sum()\n",
    "            band_obs_counts[band].append(n_obs)\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, band in enumerate(BANDS):\n",
    "        counts = band_obs_counts[band]\n",
    "        axes[idx].hist(counts, bins=50, alpha=0.7, color=f'C{idx}')\n",
    "        axes[idx].set_title(f'Band {band}')\n",
    "        axes[idx].set_xlabel('Number of observations')\n",
    "        axes[idx].set_ylabel('Count')\n",
    "        axes[idx].axvline(np.mean(counts), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(counts):.1f}')\n",
    "        axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{WORKING_DIR}/preprocessing_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nObservations per band (mean Â± std):\")\n",
    "    for band in BANDS:\n",
    "        counts = band_obs_counts[band]\n",
    "        print(f\"  {band}: {np.mean(counts):.1f} Â± {np.std(counts):.1f} \"\n",
    "              f\"(min: {np.min(counts)}, max: {np.max(counts)})\")\n",
    "    \n",
    "    # Check for completely empty objects\n",
    "    empty_count = sum(1 for obj_data in data_dict.values() \n",
    "                     if all(obj_data[b]['mask'].sum() == 0 for b in BANDS))\n",
    "    print(f\"\\nObjects with no observations in any band: {empty_count}\")\n",
    "\n",
    "print(\"Analyzing training data preprocessing:\")\n",
    "analyze_preprocessing(train_data, train_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Analyzing test data preprocessing:\")\n",
    "analyze_preprocessing(test_data, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Visualize Sample Preprocessed Light Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:53:41.164667Z",
     "iopub.status.busy": "2025-12-24T09:53:41.164339Z",
     "iopub.status.idle": "2025-12-24T09:53:46.311603Z",
     "shell.execute_reply": "2025-12-24T09:53:46.310919Z",
     "shell.execute_reply.started": "2025-12-24T09:53:41.164633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_preprocessed_lightcurve(object_id, processed_data, target=None):\n",
    "    \"\"\"\n",
    "    Visualize preprocessed light curve for all bands\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = {'u': '#6A5ACD', 'g': '#2ca02c', 'r': '#d62728', \n",
    "              'i': '#ff7f0e', 'z': '#8c564b', 'y': '#1b1b1b'}\n",
    "    \n",
    "    for idx, band in enumerate(BANDS):\n",
    "        data = processed_data[band]\n",
    "        mask = data['mask']\n",
    "        \n",
    "        # Plot real observations\n",
    "        if mask.sum() > 0:\n",
    "            axes[idx].plot(data['time'][mask], data['mag'][mask], \n",
    "                          'o-', color=colors[band], alpha=0.7, markersize=3)\n",
    "            axes[idx].errorbar(data['time'][mask], data['mag'][mask], \n",
    "                              yerr=data['mag_err'][mask], \n",
    "                              fmt='none', color=colors[band], alpha=0.3)\n",
    "        \n",
    "        # Mark padded observations\n",
    "        if (~mask).sum() > 0:\n",
    "            axes[idx].scatter(data['time'][~mask], data['mag'][~mask], \n",
    "                            marker='x', color='gray', alpha=0.3, s=20, label='Padded')\n",
    "        \n",
    "        axes[idx].set_title(f'Band {band} ({mask.sum()}/200 obs)', fontsize=12)\n",
    "        axes[idx].set_xlabel('Normalized Time')\n",
    "        axes[idx].set_ylabel('Normalized Magnitude')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        axes[idx].invert_yaxis()  # Magnitude scale inverted\n",
    "        \n",
    "        if (~mask).sum() > 0:\n",
    "            axes[idx].legend()\n",
    "    \n",
    "    title = f'Object: {object_id}'\n",
    "    if target is not None:\n",
    "        title += f' (Target: {\"TDE\" if target == 1 else \"Non-TDE\"})'\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot samples from each class\n",
    "print(\"Visualizing TDE examples:\")\n",
    "tde_samples = train_df[train_df['target'] == 1].sample(min(3, (train_df['target']==1).sum()))\n",
    "for idx, row in tde_samples.iterrows():\n",
    "    plot_preprocessed_lightcurve(row['object_id'], train_data[row['object_id']], row['target'])\n",
    "\n",
    "print(\"\\nVisualizing Non-TDE examples:\")\n",
    "non_tde_samples = train_df[train_df['target'] == 0].sample(3)\n",
    "for idx, row in non_tde_samples.iterrows():\n",
    "    plot_preprocessed_lightcurve(row['object_id'], train_data[row['object_id']], row['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Astromer Model Setup\n",
    "\n",
    "## 3.1 Download Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T10:46:23.001976Z",
     "iopub.status.busy": "2025-12-24T10:46:23.001100Z",
     "iopub.status.idle": "2025-12-24T10:46:23.022897Z",
     "shell.execute_reply": "2025-12-24T10:46:23.022284Z",
     "shell.execute_reply.started": "2025-12-24T10:46:23.001942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Astromer v1 pretrained weights from MACHO\n",
    "# v1: Best performance - mask token and residual connections\n",
    "# Performance: 0.113 loss / 0.73 R2\n",
    "\n",
    "import os\n",
    "\n",
    "# Weights path (local/vast.ai)\n",
    "WEIGHTS_DIR = \"./macho_a1/macho-clean\"\n",
    "\n",
    "# TensorFlow checkpoint files (not .h5)\n",
    "CHECKPOINT_PATH = f\"{WEIGHTS_DIR}/weights\"\n",
    "CONFIG_PATH = f\"{WEIGHTS_DIR}/config.toml\"\n",
    "\n",
    "# Verify weights exist\n",
    "print(\"Checking Astromer v1 weights...\")\n",
    "print(f\"Weights directory: {WEIGHTS_DIR}\")\n",
    "\n",
    "if os.path.exists(WEIGHTS_DIR):\n",
    "    print(f\"âœ“ Weights directory found\")\n",
    "    \n",
    "    # List files in directory\n",
    "    if os.path.isdir(WEIGHTS_DIR):\n",
    "        files = os.listdir(WEIGHTS_DIR)\n",
    "        print(f\"\\nFiles in weights directory:\")\n",
    "        for f in files:\n",
    "            file_path = os.path.join(WEIGHTS_DIR, f)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024**2)\n",
    "                print(f\"  - {f} ({size_mb:.2f} MB)\")\n",
    "            else:\n",
    "                print(f\"  - {f}/ (directory)\")\n",
    "    \n",
    "    # Check for required checkpoint files\n",
    "    required_files = [\"checkpoint\", \"weights.index\", \"weights.data-00000-of-00001\"]\n",
    "    missing = [f for f in required_files if not os.path.exists(f\"{WEIGHTS_DIR}/{f}\")]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\nâš ï¸ Missing checkpoint files: {missing}\")\n",
    "        print(\"Will train from scratch (random initialization)\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ All checkpoint files present\")\n",
    "        print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Weights directory not found: {WEIGHTS_DIR}\")\n",
    "    print(\"\\nTo use pretrained weights:\")\n",
    "    print(\"  1. Download weights from Astromer repository\")\n",
    "    print(\"  2. Place in: ./macho_a1/macho-clean/\")\n",
    "    print(\"  3. Required files: checkpoint, weights.index, weights.data-*\")\n",
    "    print(\"\\nWill train from scratch if weights not found\")\n",
    "\n",
    "print(\"\\nModel specs (v1):\")\n",
    "print(\"  - Architecture: 6-layer transformer\")\n",
    "print(\"  - Attention heads: 4 per layer\")\n",
    "print(\"  - Embedding dimension: 256\")\n",
    "print(\"  - Pretrained on: MACHO dataset (1.5M light curves)\")\n",
    "print(\"  - Features: Mask token + residual connections\")\n",
    "print(\"  - Format: TensorFlow checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load and Verify Astromer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T11:03:28.307131Z",
     "iopub.status.busy": "2025-12-24T11:03:28.306561Z",
     "iopub.status.idle": "2025-12-24T11:03:28.387561Z",
     "shell.execute_reply": "2025-12-24T11:03:28.386399Z",
     "shell.execute_reply.started": "2025-12-24T11:03:28.307102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load and verify Astromer model using official library\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import toml\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING ASTROMER MODEL (Official Method)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add Astromer to Python path\n",
    "ASTROMER_PATH = os.path.abspath(\"./astromer_repo/src\")\n",
    "if ASTROMER_PATH not in sys.path:\n",
    "    sys.path.insert(0, ASTROMER_PATH)\n",
    "    print(f\"âœ“ Added to sys.path: {ASTROMER_PATH}\")\n",
    "\n",
    "# Import Astromer\n",
    "try:\n",
    "    from models.astromer_1 import get_ASTROMER\n",
    "    print(\"âœ“ Astromer module imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import Astromer: {e}\")\n",
    "    print(\"Make sure astromer_repo is cloned and in the correct location\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nðŸ“¦ Environment:\")\n",
    "import tensorflow as tf\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Load configuration from checkpoint\n",
    "print(\"\\nðŸ“ Loading model configuration...\")\n",
    "CONFIG_PATH = \"./macho_a1/macho-clean/config.toml\"\n",
    "WEIGHTS_PATH = \"./macho_a1/macho-clean/weights\"\n",
    "\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = toml.load(f)\n",
    "    \n",
    "    print(\"âœ“ Config loaded from checkpoint\")\n",
    "    print(f\"  Architecture: {config['num_layers']} layers, {config['num_heads']} heads\")\n",
    "    print(f\"  Model dimension: {config['pe_dim']}\")\n",
    "    print(f\"  Window size: {config['window_size']}\")\n",
    "    \n",
    "    # Build model using official function\n",
    "    print(\"\\nðŸ—ï¸  Building Astromer model...\")\n",
    "    astromer_model = get_ASTROMER(\n",
    "        num_layers=config['num_layers'],\n",
    "        num_heads=config['num_heads'],\n",
    "        head_dim=config['head_dim'],\n",
    "        mixer_size=config['mixer'],\n",
    "        dropout=config['dropout'],\n",
    "        pe_base=config['pe_base'],\n",
    "        pe_dim=config['pe_dim'],\n",
    "        window_size=config['window_size'],\n",
    "        trainable_mask=True,  # Use MSK token as in v1\n",
    "        mask_format='Q'       # Astromer v1 format\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Config file not found, using default architecture\")\n",
    "    print(\"\\nðŸ—ï¸  Building Astromer model with default config...\")\n",
    "    astromer_model = get_ASTROMER(\n",
    "        num_layers=6,\n",
    "        num_heads=4,\n",
    "        head_dim=64,\n",
    "        mixer_size=128,\n",
    "        dropout=0.1,\n",
    "        pe_base=10000,\n",
    "        pe_dim=256,\n",
    "        window_size=200,\n",
    "        trainable_mask=True,\n",
    "        mask_format='Q'\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Model built successfully\")\n",
    "print(f\"  Model name: {astromer_model.name}\")\n",
    "print(f\"  Total parameters: {astromer_model.count_params():,}\")\n",
    "\n",
    "# Test forward pass with random data\n",
    "print(\"\\nðŸ§ª Testing forward pass...\")\n",
    "dummy_input = {\n",
    "    'input': np.random.randn(2, 200, 1).astype(np.float32),\n",
    "    'times': np.random.randn(2, 200, 1).astype(np.float32),\n",
    "    'mask_in': np.ones((2, 200, 1), dtype=np.float32)\n",
    "}\n",
    "\n",
    "try:\n",
    "    output = astromer_model(dummy_input, training=False)\n",
    "    print(\"âœ“ Forward pass successful!\")\n",
    "    print(f\"  Input shape: {dummy_input['input'].shape}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Output range: [{output.numpy().min():.3f}, {output.numpy().max():.3f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Forward pass failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Load pretrained weights\n",
    "if os.path.exists(WEIGHTS_PATH + \".index\"):\n",
    "    print(\"\\nðŸ“¦ Loading pretrained weights...\")\n",
    "    try:\n",
    "        status = astromer_model.load_weights(WEIGHTS_PATH).expect_partial()\n",
    "        print(\"âœ“ Pretrained weights loaded successfully\")\n",
    "        print(\"  Using .expect_partial() for compatibility\")\n",
    "        \n",
    "        # Test again with loaded weights\n",
    "        output_loaded = astromer_model(dummy_input, training=False)\n",
    "        print(f\"  Output range (with weights): [{output_loaded.numpy().min():.3f}, {output_loaded.numpy().max():.3f}]\")\n",
    "        \n",
    "        # Check if weights actually loaded (output should be different)\n",
    "        if not np.allclose(output.numpy(), output_loaded.numpy()):\n",
    "            print(\"âœ“ Weights loaded correctly (model behavior changed)\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Warning: Model output unchanged - weights may not have loaded\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load pretrained weights: {e}\")\n",
    "        print(\"Will proceed with random initialization (train from scratch)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Pretrained weights not found\")\n",
    "    print(\"Will train from scratch with random initialization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL READY FOR FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ Summary:\")\n",
    "print(\"  âœ“ Astromer model loaded using official library\")\n",
    "print(\"  âœ“ Architecture: 6-layer transformer (256 dims)\")\n",
    "print(\"  âœ“ Forward pass working correctly\")\n",
    "if os.path.exists(WEIGHTS_PATH + \".index\"):\n",
    "    print(\"  âœ“ Pretrained weights loaded from MACHO dataset\")\n",
    "else:\n",
    "    print(\"  âš ï¸  Using random initialization\")\n",
    "print(\"\\nReady to proceed to fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Fine-tuning Astromer (Per Band)\n",
    "\n",
    "Train separate Astromer models for each band (u, g, r, i, z, y) to learn band-specific temporal patterns through magnitude reconstruction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build Astromer Architecture (Exact Match with Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:23:45.994643Z",
     "iopub.status.busy": "2025-12-24T09:23:45.993957Z",
     "iopub.status.idle": "2025-12-24T09:23:46.314864Z",
     "shell.execute_reply": "2025-12-24T09:23:46.314166Z",
     "shell.execute_reply.started": "2025-12-24T09:23:45.994614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build Astromer v1 architecture from scratch using TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding for time series\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=256, max_len=200, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Create positional encoding matrix\n",
    "        position = np.arange(self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))\n",
    "        \n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Single transformer block with multi-head attention\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=256, num_heads=4, ff_dim=128, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Multi-head self-attention\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.d_model // self.num_heads,\n",
    "            dropout=0.0  # mhsa_dropout\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(self.ff_dim, activation='tanh'),\n",
    "            layers.Dense(self.d_model)\n",
    "        ])\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def call(self, x, mask=None, training=False):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.mha(x, x, attention_mask=mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "def build_astromer_model(config):\n",
    "    \"\"\"\n",
    "    Build Astromer v1 model for magnitude reconstruction\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary with model parameters\n",
    "    \n",
    "    Returns:\n",
    "        Keras model\n",
    "    \"\"\"\n",
    "    max_obs = config['max_obs']\n",
    "    d_model = config['output_dim']\n",
    "    num_layers = config['num_layers']\n",
    "    num_heads = config['num_heads']\n",
    "    ff_dim = config['ff_dim']\n",
    "    dropout = config['dropout']\n",
    "    \n",
    "    # Input: [time, magnitude, magnitude_error]\n",
    "    inputs = keras.Input(shape=(max_obs, 3), name='input_lc')\n",
    "    \n",
    "    # Linear projection to d_model dimensions\n",
    "    x = layers.Dense(d_model, name='input_projection')(inputs)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    x = PositionalEncoding(d_model=d_model, max_len=max_obs, name='pos_encoding')(x)\n",
    "    \n",
    "    # Stack transformer blocks\n",
    "    for i in range(num_layers):\n",
    "        x = TransformerBlock(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout=dropout,\n",
    "            name=f'transformer_block_{i}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output layer for magnitude reconstruction\n",
    "    outputs = layers.Dense(1, name='magnitude_output')(x)\n",
    "    \n",
    "    # Squeeze last dimension: (batch, seq, 1) -> (batch, seq)\n",
    "    # Use Lambda layer to wrap TensorFlow operation in Keras Functional API\n",
    "    outputs = layers.Lambda(lambda x: tf.squeeze(x, axis=-1), name='squeeze_output')(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='astromer_v1')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "print(\"Building Astromer v1 model...\")\n",
    "astromer_model = build_astromer_model(ASTROMER_CONFIG)\n",
    "\n",
    "print(f\"âœ“ Model built successfully\")\n",
    "print(f\"\\nModel summary:\")\n",
    "astromer_model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {astromer_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:23:59.840554Z",
     "iopub.status.busy": "2025-12-24T09:23:59.839691Z",
     "iopub.status.idle": "2025-12-24T09:23:59.857566Z",
     "shell.execute_reply": "2025-12-24T09:23:59.856700Z",
     "shell.execute_reply.started": "2025-12-24T09:23:59.840524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data generator for Astromer training\n",
    "class AstromerDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generate batches of light curve data for Astromer training\n",
    "    Uses magnitude reconstruction task with masking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dict, object_ids, band, batch_size=32, \n",
    "                 mask_fraction=0.5, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict: Dictionary mapping object_id -> preprocessed data\n",
    "            object_ids: List of object IDs to use\n",
    "            band: Which band to use ('u', 'g', 'r', 'i', 'z', 'y')\n",
    "            batch_size: Batch size\n",
    "            mask_fraction: Fraction of observations to mask for prediction\n",
    "            shuffle: Whether to shuffle data each epoch\n",
    "        \"\"\"\n",
    "        self.data_dict = data_dict\n",
    "        self.object_ids = object_ids\n",
    "        self.band = band\n",
    "        self.batch_size = batch_size\n",
    "        self.mask_fraction = mask_fraction\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Filter objects that have data in this band\n",
    "        self.valid_ids = []\n",
    "        for obj_id in self.object_ids:\n",
    "            if obj_id in self.data_dict:\n",
    "                n_obs = self.data_dict[obj_id][band]['mask'].sum()\n",
    "                if n_obs >= 10:  # Require at least 10 real observations\n",
    "                    self.valid_ids.append(obj_id)\n",
    "        \n",
    "        print(f\"Band {band}: {len(self.valid_ids)}/{len(object_ids)} objects have sufficient data\")\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.valid_ids) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch of object IDs\n",
    "        batch_ids = self.valid_ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Prepare batch\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        mask_batch = []\n",
    "        \n",
    "        for obj_id in batch_ids:\n",
    "            data = self.data_dict[obj_id][self.band]\n",
    "            \n",
    "            time = data['time']\n",
    "            mag = data['mag']\n",
    "            mag_err = data['mag_err']\n",
    "            real_mask = data['mask']\n",
    "            \n",
    "            # Stack as input: [time, magnitude, magnitude_error]\n",
    "            lc_input = np.stack([time, mag, mag_err], axis=-1)\n",
    "            \n",
    "            # Target: original magnitudes\n",
    "            lc_target = mag.copy()\n",
    "            \n",
    "            # Create mask for reconstruction (only on real observations)\n",
    "            recon_mask = real_mask.astype(np.float32)\n",
    "            \n",
    "            X_batch.append(lc_input)\n",
    "            y_batch.append(lc_target)\n",
    "            mask_batch.append(recon_mask)\n",
    "        \n",
    "        X_batch = np.array(X_batch, dtype=np.float32)\n",
    "        y_batch = np.array(y_batch, dtype=np.float32)\n",
    "        mask_batch = np.array(mask_batch, dtype=np.float32)\n",
    "        \n",
    "        return X_batch, (y_batch, mask_batch)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_ids)\n",
    "\n",
    "# Test data generator\n",
    "print(\"Testing data generator...\")\n",
    "test_gen = AstromerDataGenerator(\n",
    "    train_data, \n",
    "    list(train_data.keys())[:100], \n",
    "    band='r',  # Start with r-band (most observations)\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Generator length: {len(test_gen)} batches\")\n",
    "\n",
    "# Get sample batch\n",
    "X_sample, (y_sample, mask_sample) = test_gen[0]\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  X: {X_sample.shape}\")\n",
    "print(f\"  y: {y_sample.shape}\")\n",
    "print(f\"  mask: {mask_sample.shape}\")\n",
    "print(f\"  Real observations: {mask_sample.sum():.0f}/{mask_sample.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Define Training Loss and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:24:05.565631Z",
     "iopub.status.busy": "2025-12-24T09:24:05.565345Z",
     "iopub.status.idle": "2025-12-24T09:24:05.576135Z",
     "shell.execute_reply": "2025-12-24T09:24:05.575315Z",
     "shell.execute_reply.started": "2025-12-24T09:24:05.565607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom loss function for masked magnitude reconstruction\n",
    "@tf.function\n",
    "def masked_mse_loss(y_true, y_pred, mask):\n",
    "    \"\"\"\n",
    "    Compute MSE loss only on real (non-padded) observations\n",
    "    \n",
    "    Args:\n",
    "        y_true: True magnitudes (batch, seq_len)\n",
    "        y_pred: Predicted magnitudes (batch, seq_len)\n",
    "        mask: Binary mask (batch, seq_len) - 1 for real, 0 for padded\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss\n",
    "    \"\"\"\n",
    "    # Compute squared errors\n",
    "    squared_errors = tf.square(y_true - y_pred)\n",
    "    \n",
    "    # Apply mask\n",
    "    masked_errors = squared_errors * mask\n",
    "    \n",
    "    # Compute mean over real observations\n",
    "    n_real = tf.reduce_sum(mask)\n",
    "    loss = tf.reduce_sum(masked_errors) / (n_real + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def masked_mae_metric(y_true, y_pred, mask):\n",
    "    \"\"\"Compute MAE only on real observations\"\"\"\n",
    "    abs_errors = tf.abs(y_true - y_pred)\n",
    "    masked_errors = abs_errors * mask\n",
    "    n_real = tf.reduce_sum(mask)\n",
    "    return tf.reduce_sum(masked_errors) / (n_real + 1e-8)\n",
    "\n",
    "# Wrapper for Keras\n",
    "class MaskedMSELoss(keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        # In Keras 3, when generator returns (y, mask), y_true is a list/tuple\n",
    "        if isinstance(y_true, (list, tuple)):\n",
    "            y_true_vals, mask = y_true\n",
    "        else:\n",
    "            # Fallback if not a tuple\n",
    "            y_true_vals = y_true\n",
    "            mask = tf.ones_like(y_true)\n",
    "        \n",
    "        return masked_mse_loss(y_true_vals, y_pred, mask)\n",
    "\n",
    "class MaskedMAE(keras.metrics.Metric):\n",
    "    def __init__(self, name='masked_mae', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Handle tuple unpacking\n",
    "        if isinstance(y_true, (list, tuple)):\n",
    "            y_true_vals, mask = y_true\n",
    "        else:\n",
    "            y_true_vals = y_true\n",
    "            mask = tf.ones_like(y_true)\n",
    "        \n",
    "        mae = masked_mae_metric(y_true_vals, y_pred, mask)\n",
    "        self.total.assign_add(mae)\n",
    "        self.count.assign_add(1.0)\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "print(\"âœ“ Custom loss and metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fine-tune Single Band (Test Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:24:11.344600Z",
     "iopub.status.busy": "2025-12-24T09:24:11.343979Z",
     "iopub.status.idle": "2025-12-24T09:24:12.082417Z",
     "shell.execute_reply": "2025-12-24T09:24:12.081446Z",
     "shell.execute_reply.started": "2025-12-24T09:24:11.344569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test fine-tuning on r-band (most observations)\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST FINE-TUNING: Band r\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create data generators\n",
    "train_ids = list(train_data.keys())\n",
    "val_split = int(0.9 * len(train_ids))\n",
    "\n",
    "train_gen = AstromerDataGenerator(\n",
    "    train_data,\n",
    "    train_ids[:val_split],\n",
    "    band='r',\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = AstromerDataGenerator(\n",
    "    train_data,\n",
    "    train_ids[val_split:],\n",
    "    band='r',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Build fresh model\n",
    "model_r = build_astromer_model(ASTROMER_CONFIG)\n",
    "\n",
    "# Try to load pretrained weights using TensorFlow checkpoint API\n",
    "print(\"\\nAttempting to load pretrained weights...\")\n",
    "try:\n",
    "    # Method 1: Try standard Keras load (works if weights format matches)\n",
    "    model_r.load_weights(CHECKPOINT_PATH)\n",
    "    print(\"âœ“ Pretrained weights loaded successfully (Keras API)\")\n",
    "except Exception as e1:\n",
    "    print(f\"âš ï¸ Keras API failed: {e1}\")\n",
    "    \n",
    "    # Method 2: Try TensorFlow Checkpoint API (for old TF checkpoint format)\n",
    "    try:\n",
    "        print(\"\\nTrying TensorFlow Checkpoint API...\")\n",
    "        \n",
    "        # Build a dummy input to initialize model weights\n",
    "        dummy_input = np.random.randn(1, MAX_OBSERVATIONS, 3).astype(np.float32)\n",
    "        _ = model_r(dummy_input, training=False)\n",
    "        \n",
    "        # Create checkpoint object\n",
    "        checkpoint = tf.train.Checkpoint(model=model_r)\n",
    "        \n",
    "        # Restore from checkpoint\n",
    "        status = checkpoint.restore(CHECKPOINT_PATH)\n",
    "        \n",
    "        # Check status\n",
    "        try:\n",
    "            status.assert_existing_objects_matched()\n",
    "            print(\"âœ“ Pretrained weights loaded successfully (TF Checkpoint API)\")\n",
    "        except:\n",
    "            print(\"âš ï¸ Partial match - some weights loaded, some initialized randomly\")\n",
    "            print(\"This is expected if architecture differs slightly from pretrained model\")\n",
    "    \n",
    "    except Exception as e2:\n",
    "        print(f\"âš ï¸ TensorFlow Checkpoint API also failed: {e2}\")\n",
    "        print(\"Training from scratch...\")\n",
    "\n",
    "# Compile model\n",
    "model_r.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=MaskedMSELoss(),\n",
    "    metrics=[MaskedMAE()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{CHECKPOINTS_DIR}/astromer_r_best.weights.h5\",\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Fine-tune (just a few epochs for testing)\n",
    "print(\"\\nStarting fine-tuning...\")\n",
    "history = model_r.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=3,  # Start with just 3 epochs to test\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Test fine-tuning completed\")\n",
    "print(f\"Final train loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9095929,
     "sourceId": 14255534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9112781,
     "sourceId": 14278308,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
